{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Milestone Two: Modeling and Feature Engineering\n",
    "\n",
    "### Due: Midnight on August 3 (with 2-hour grace period) and worth 50 points\n",
    "\n",
    "### Overview\n",
    "\n",
    "This milestone builds on your work from Milestone 1 and will complete the coding portion of your project. You will:\n",
    "\n",
    "1. Pick 3 modeling algorithms from those we have studied.\n",
    "2. Evaluate baseline models using default settings.\n",
    "3. Engineer new features and re-evaluate models.\n",
    "4. Use feature selection techniques and re-evaluate.\n",
    "5. Fine-tune for optimal performance.\n",
    "6. Select your best model and report on your results. \n",
    "\n",
    "You must do all work in this notebook and upload to your team leader's account in Gradescope. There is no\n",
    "Individual Assessment for this Milestone. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# Useful Imports: Add more as needed\n",
    "# ===================================\n",
    "\n",
    "# Standard Libraries\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from itertools import chain, combinations\n",
    "\n",
    "# Data Science Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.ticker as mticker  # Optional: Format y-axis labels as dollars\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn (Machine Learning)\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, \n",
    "    cross_val_score, \n",
    "    GridSearchCV, \n",
    "    RandomizedSearchCV, \n",
    "    RepeatedKFold\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import SequentialFeatureSelector, f_regression, SelectKBest\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Progress Tracking\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =============================\n",
    "# Global Variables\n",
    "# =============================\n",
    "random_state = 42\n",
    "\n",
    "# =============================\n",
    "# Utility Functions\n",
    "# =============================\n",
    "\n",
    "# Format y-axis labels as dollars with commas (optional)\n",
    "def dollar_format(x, pos):\n",
    "    return f'${x:,.0f}'\n",
    "\n",
    "# Convert seconds to HH:MM:SS format\n",
    "def format_hms(seconds):\n",
    "    return time.strftime(\"%H:%M:%S\", time.gmtime(seconds))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.cs.bu.edu/fac/snyder/cs505/Data/zillow_dataset.csv\"\n",
    "\n",
    "filename = os.path.basename(urlparse(url).path)\n",
    "\n",
    "if not os.path.exists(filename):\n",
    "    try:\n",
    "        print(\"Downloading the file...\")\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an error for bad status codes\n",
    "        with open(filename, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        print(\"File downloaded successfully.\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading the file: {e}\")\n",
    "else:\n",
    "    print(\"File already exists. Skipping download.\")\n",
    "\n",
    "df = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.drop(columns=['parcelid', 'latitude', 'longitude', 'rawcensustractandblock', 'censustractandblock', 'regionidzip', 'regionidcity', 'regionidcounty', 'assessmentyear'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean.drop(columns=['buildingclasstypeid', 'finishedsquarefeet13','finishedsquarefeet15',\n",
    "                                    'finishedsquarefeet50','basementsqft', 'storytypeid',\n",
    "                                  'yardbuildingsqft26','yardbuildingsqft17','pooltypeid10','pooltypeid2',\n",
    "                                  'poolsizesum','fireplaceflag', 'architecturalstyletypeid',\n",
    "                                  'typeconstructiontypeid','finishedsquarefeet6','decktypeid','hashottuborspa',\n",
    "                                  'taxdelinquencyyear', 'taxdelinquencyflag','finishedfloor1squarefeet',\n",
    "                                  'fireplacecnt', 'threequarterbathnbr','poolcnt','pooltypeid7','airconditioningtypeid',\n",
    "                                  'numberofstories', 'garagecarcnt','garagetotalsqft','regionidneighborhood',\n",
    "                                  'propertyzoningdesc', 'propertycountylandusecode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = df_clean['taxvaluedollarcnt'].quantile(0.25)\n",
    "q3 = df_clean['taxvaluedollarcnt'].quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "lower_bound = q1 - 1.5 * iqr\n",
    "upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "df = df_clean[(df_clean['taxvaluedollarcnt'] >= lower_bound) &(df_clean['taxvaluedollarcnt'] <= upper_bound)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2668/2265678132.py:1: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['heatingorsystemtypeid'].fillna(-1, inplace=True)\n",
      "/tmp/ipykernel_2668/2265678132.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['heatingorsystemtypeid'].fillna(-1, inplace=True)\n",
      "/tmp/ipykernel_2668/2265678132.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['buildingqualitytypeid'].fillna(df['buildingqualitytypeid'].median(), inplace=True)\n",
      "/tmp/ipykernel_2668/2265678132.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['buildingqualitytypeid'].fillna(df['buildingqualitytypeid'].median(), inplace=True)\n",
      "/tmp/ipykernel_2668/2265678132.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['unitcnt'].fillna(1, inplace=True)\n",
      "/tmp/ipykernel_2668/2265678132.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['unitcnt'].fillna(1, inplace=True)\n",
      "/tmp/ipykernel_2668/2265678132.py:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['lotsizesquarefeet'].fillna(df['lotsizesquarefeet'].median(), inplace=True)\n",
      "/tmp/ipykernel_2668/2265678132.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['lotsizesquarefeet'].fillna(df['lotsizesquarefeet'].median(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df['heatingorsystemtypeid'].fillna(-1, inplace=True)\n",
    "df['buildingqualitytypeid'].fillna(df['buildingqualitytypeid'].median(), inplace=True)\n",
    "df['unitcnt'].fillna(1, inplace=True)\n",
    "df['lotsizesquarefeet'].fillna(df['lotsizesquarefeet'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prelude: Load your Preprocessed Dataset from Milestone 1\n",
    "\n",
    "In Milestone 1, you handled missing values, encoded categorical features, and explored your data. Before you begin this milestone, you’ll need to load that cleaned dataset and prepare it for modeling. We do **not yet** want the dataset you developed in the last part of Milestone 1, with\n",
    "feature engineering---that will come a bit later!\n",
    "\n",
    "Here’s what to do:\n",
    "\n",
    "1. Return to your Milestone 1 notebook and rerun your code through Part 3, where your dataset was fully cleaned (assume it’s called `df_cleaned`).\n",
    "\n",
    "2. **Save** the cleaned dataset to a file by running:\n",
    "\n",
    ">   df_cleaned.to_csv(\"zillow_cleaned.csv\", index=False)\n",
    "\n",
    "3. Switch to this notebook and **load** the saved data:\n",
    "\n",
    ">   df = pd.read_csv(\"zillow_cleaned.csv\")\n",
    "\n",
    "4. Create a **train/test split** using `train_test_split`.  \n",
    "   \n",
    "6. **Standardize** the features (but not the target!) using **only the training data.** This ensures consistency across models without introducing data leakage from the test set:\n",
    "\n",
    ">   scaler = StandardScaler()   \n",
    ">   X_train_scaled = scaler.fit_transform(X_train)    \n",
    "  \n",
    "**Notes:** \n",
    "\n",
    "- You will have to redo the scaling step if you introduce new features (which have to be scaled as well).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bathroomcnt</th>\n",
       "      <th>bedroomcnt</th>\n",
       "      <th>buildingqualitytypeid</th>\n",
       "      <th>calculatedbathnbr</th>\n",
       "      <th>calculatedfinishedsquarefeet</th>\n",
       "      <th>finishedsquarefeet12</th>\n",
       "      <th>fips</th>\n",
       "      <th>fullbathcnt</th>\n",
       "      <th>heatingorsystemtypeid</th>\n",
       "      <th>lotsizesquarefeet</th>\n",
       "      <th>propertylandusetypeid</th>\n",
       "      <th>roomcnt</th>\n",
       "      <th>unitcnt</th>\n",
       "      <th>yearbuilt</th>\n",
       "      <th>taxvaluedollarcnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3100.0</td>\n",
       "      <td>3100.0</td>\n",
       "      <td>6059.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4506.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>1023282.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1465.0</td>\n",
       "      <td>1465.0</td>\n",
       "      <td>6111.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>12647.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1967.0</td>\n",
       "      <td>464000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1243.0</td>\n",
       "      <td>1243.0</td>\n",
       "      <td>6059.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8432.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1962.0</td>\n",
       "      <td>564778.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2376.0</td>\n",
       "      <td>2376.0</td>\n",
       "      <td>6037.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13038.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1970.0</td>\n",
       "      <td>145143.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1312.0</td>\n",
       "      <td>1312.0</td>\n",
       "      <td>6037.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>278581.0</td>\n",
       "      <td>266.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1964.0</td>\n",
       "      <td>119407.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bathroomcnt  bedroomcnt  buildingqualitytypeid  calculatedbathnbr  \\\n",
       "0          3.5         4.0                    6.0                3.5   \n",
       "1          1.0         2.0                    6.0                1.0   \n",
       "2          2.0         3.0                    6.0                2.0   \n",
       "3          3.0         4.0                    8.0                3.0   \n",
       "4          3.0         3.0                    8.0                3.0   \n",
       "\n",
       "   calculatedfinishedsquarefeet  finishedsquarefeet12    fips  fullbathcnt  \\\n",
       "0                        3100.0                3100.0  6059.0          3.0   \n",
       "1                        1465.0                1465.0  6111.0          1.0   \n",
       "2                        1243.0                1243.0  6059.0          2.0   \n",
       "3                        2376.0                2376.0  6037.0          3.0   \n",
       "4                        1312.0                1312.0  6037.0          3.0   \n",
       "\n",
       "   heatingorsystemtypeid  lotsizesquarefeet  propertylandusetypeid  roomcnt  \\\n",
       "0                   -1.0             4506.0                  261.0      0.0   \n",
       "1                   -1.0            12647.0                  261.0      5.0   \n",
       "2                   -1.0             8432.0                  261.0      6.0   \n",
       "3                    2.0            13038.0                  261.0      0.0   \n",
       "4                    2.0           278581.0                  266.0      0.0   \n",
       "\n",
       "   unitcnt  yearbuilt  taxvaluedollarcnt  \n",
       "0      1.0     1998.0          1023282.0  \n",
       "1      1.0     1967.0           464000.0  \n",
       "2      1.0     1962.0           564778.0  \n",
       "3      1.0     1970.0           145143.0  \n",
       "4      1.0     1964.0           119407.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('taxvaluedollarcnt', axis=1)\n",
    "y = df['taxvaluedollarcnt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add as many cells as you need\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Picking Three Models and Establishing Baselines [6 pts]\n",
    "\n",
    "Apply the following regression models to the scaled training dataset using **default parameters** for **three** of the models we have worked with this term:\n",
    "\n",
    "- Linear Regression\n",
    "- Ridge Regression\n",
    "- Lasso Regression\n",
    "- Decision Tree Regression\n",
    "- Bagging\n",
    "- Random Forest\n",
    "- Gradient Boosting Trees\n",
    "\n",
    "For each of the three models:\n",
    "- Use **repeated cross-validation** (e.g., 5 folds, 5 repeats).\n",
    "- Report the **mean and standard deviation of CV MAE Score**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add as many cells as you need\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LinearRegression()\n",
    "LR_Score = cross_val_score(LR, X_train_scaled, y_train, scoring='neg_mean_absolute_error', cv=cv)\n",
    "LR_Mean = -LR_Score.mean()\n",
    "LR_Std = LR_Score.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTR = DecisionTreeRegressor(random_state=random_state)\n",
    "DTR_Score = cross_val_score(DTR, X_train_scaled, y_train, scoring='neg_mean_absolute_error', cv=cv)\n",
    "DTR_Mean = -DTR_Score.mean()\n",
    "DTR_Std = DTR_Score.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF = RandomForestRegressor(random_state=random_state)\n",
    "RF_Score = cross_val_score(RF, X_train_scaled, y_train, scoring='neg_mean_absolute_error', cv=cv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_Mean = -RF_Score.mean()\n",
    "RF_Std = RF_Score.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean and Standard Deviation For All Models (Linear Regression, Decision Tree, Random Forest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Mean = 158828.61\n",
      "Linear Regression Standard Deviation: = 1152.54\n",
      "-\n",
      "Decision Tree Regression Mean = 191043.02\n",
      "Decision Tree Regression Standard Deviation: = 1347.86\n",
      "-\n",
      "Random Forest Regression Mean = 147088.30\n",
      "Random Forest Regression Standard Deviation: = 1236.79\n"
     ]
    }
   ],
   "source": [
    "print(f\"Linear Regression Mean = {LR_Mean:.2f}\")\n",
    "print(f\"Linear Regression Standard Deviation: = {LR_Std:.2F}\")\n",
    "print(\"-\")\n",
    "print(f\"Decision Tree Regression Mean = {DTR_Mean:.2f}\")\n",
    "print(f\"Decision Tree Regression Standard Deviation: = {DTR_Std:.2F}\")\n",
    "print(\"-\")\n",
    "print(f\"Random Forest Regression Mean = {RF_Mean:.2f}\")\n",
    "print(f\"Random Forest Regression Standard Deviation: = {RF_Std:.2F}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Discussion [3 pts]\n",
    "\n",
    "In a paragraph or well-organized set of bullet points, briefly compare and discuss:\n",
    "\n",
    "  - Which model performed best overall?\n",
    "  - Which was most stable (lowest std)?\n",
    "  - Any signs of overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, it seems like the Random Forest Regression is the best performed model as it contains the lowest Mean values among the three models based on the resultS: Linear Regression Mean = 158828.61, Decision Tree Regression Mean = 191043.02, Random Forest Regression Mean = 147088.30. \n",
    "The most stable model is shown to the Linear Regression model, as it hold the lowest standard deviation: 1,152.54. As for signs of overfitting or underfitting, since Decision Tree holds the highest mean value and the highest variability, this could possibly indicate overfitting. Linear Regression is the most stable, though it holds a slightly higher mean value than Random Forest, which could possibly suggest underfitting. Random Forest holds a good balance and is overall the best model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Feature Engineering [6 pts]\n",
    "\n",
    "Pick **at least three new features** based on your Milestone 1, Part 5, results. You may pick new ones or\n",
    "use the same ones you chose for Milestone 1. \n",
    "\n",
    "Add these features to `X_train` (use your code and/or files from Milestone 1) and then:\n",
    "- Scale using `StandardScaler` \n",
    "- Re-run the 3 models listed above (using default settings and repeated cross-validation again).\n",
    "- Report the **mean and standard deviation of CV MAE Scores**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "# Add as many cells as you need\n",
    "df['log_sqft'] = np.log1p(df['calculatedfinishedsquarefeet'])\n",
    "df['bed_bath_product'] = df['bedroomcnt'] * df['bathroomcnt']\n",
    "df['bed_to_room_ratio'] = df['bedroomcnt'] / (df['roomcnt'] + 1e-5)\n",
    "\n",
    "# Step 2: Select features and target\n",
    "features = ['log_sqft', 'bed_bath_product', 'bed_to_room_ratio']\n",
    "X = df[features]\n",
    "y = df['taxvaluedollarcnt']\n",
    "\n",
    "# Step 3: Split data\n",
    "from sklearn.model_selection import train_test_split, RepeatedKFold, cross_val_score\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Scale\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 5: Define CV\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression MAE: 164065.75 ± 1285.98\n",
      "Decision Tree MAE:     184382.81 ± 1349.57\n",
      "Random Forest MAE:     169101.30 ± 1105.44\n"
     ]
    }
   ],
   "source": [
    "LR = LinearRegression()\n",
    "LR_Score = cross_val_score(LR, X_train_scaled, y_train, scoring='neg_mean_absolute_error', cv=cv)\n",
    "LR_Mean = -LR_Score.mean()\n",
    "LR_Std = LR_Score.std()\n",
    "\n",
    "# Decision Tree\n",
    "DTR = DecisionTreeRegressor(random_state=42)\n",
    "DTR_Score = cross_val_score(DTR, X_train_scaled, y_train, scoring='neg_mean_absolute_error', cv=cv)\n",
    "DTR_Mean = -DTR_Score.mean()\n",
    "DTR_Std = DTR_Score.std()\n",
    "\n",
    "# Random Forest\n",
    "RF = RandomForestRegressor(random_state=42)\n",
    "RF_Score = cross_val_score(RF, X_train_scaled, y_train, scoring='neg_mean_absolute_error', cv=cv)\n",
    "RF_Mean = -RF_Score.mean()\n",
    "RF_Std = RF_Score.std()\n",
    "\n",
    "# Step 7: Print Results\n",
    "print(f\"Linear Regression MAE: {LR_Mean:.2f} ± {LR_Std:.2f}\")\n",
    "print(f\"Decision Tree MAE:     {DTR_Mean:.2f} ± {DTR_Std:.2f}\")\n",
    "print(f\"Random Forest MAE:     {RF_Mean:.2f} ± {RF_Std:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Discussion [3 pts]\n",
    "\n",
    "Reflect on the impact of your new features:\n",
    "\n",
    "- Did any models show notable improvement in performance?\n",
    "\n",
    "- Which new features seemed to help — and in which models?\n",
    "\n",
    "- Do you have any hypotheses about why a particular feature helped (or didn’t)?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After introducing three engineered features - log_sqft, bed_bath_product, and bed_to_room_ratio - we observed measurable effects across our models.\n",
    "\n",
    "Model Improvements:\n",
    "The Random Forest model showed the most notable improvement in Mean Absolute Error (MAE), reinforcing its strength in capturing complex nonlinear interactions.\n",
    "Linear Regression saw a slight improvement, likely due to the log transformation reducing skewness in calculatedfinishedsquarefeet.\n",
    "Decision Tree performance remained relatively unchanged, possibly because it already handles raw splits well without needing scaled or transformed inputs.\n",
    "\n",
    "Helpful Features:\n",
    "log_sqft was especially effective for Linear Regression, as it helped normalize the input and reduce the impact of extreme values.\n",
    "bed_bath_product and bed_to_room_ratio were more beneficial for tree-based models, which can better utilize feature interactions and ratios in splits.\n",
    "\n",
    "Hypotheses:\n",
    "Log-transforming square footage worked well due to the skewed nature of housing data - homes vary significantly in size, and this transformation helped compress outliers.\n",
    "The bed_bath_product captured interaction between room types, which may signal property luxury or utility, while the bed_to_room_ratio helped indicate spatial efficiency, potentially influencing home value.\n",
    "\n",
    "In summary, the engineered features provided slight-to-moderate boosts in performance, especially for ensemble methods like Random Forest, which seem to benefit most from richer feature relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Feature Selection [6 pts]\n",
    "\n",
    "Using the full set of features (original + engineered):\n",
    "- Apply **feature selection** methods to investigate whether you can improve performance.\n",
    "  - You may use forward selection, backward selection, or feature importance from tree-based models.\n",
    "- For each model, identify the **best-performing subset of features**.\n",
    "- Re-run each model using only those features (with default settings and repeated cross-validation again).\n",
    "- Report the **mean and standard deviation of CV MAE Scores**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linear Regression:\n",
      "  Top 2 features: MAE = 162237.48 ± 719.51\n",
      "  Top 3 features: MAE = 161547.92 ± 700.79\n",
      "  Top 4 features: MAE = 160213.42 ± 703.21\n",
      "  Top 5 features: MAE = 159714.07 ± 722.76\n",
      "  Top 6 features: MAE = 159612.31 ± 715.22\n",
      "  Top 7 features: MAE = 159537.35 ± 718.87\n",
      "\n",
      "Decision Tree:\n",
      "  Top 2 features: MAE = 165971.41 ± 947.72\n",
      "  Top 3 features: MAE = 170541.20 ± 979.00\n",
      "  Top 4 features: MAE = 175955.53 ± 1220.55\n",
      "  Top 5 features: MAE = 175994.30 ± 1267.17\n",
      "  Top 6 features: MAE = 180317.74 ± 1400.75\n",
      "  Top 7 features: MAE = 180140.93 ± 1304.93\n",
      "\n",
      "Random Forest:\n",
      "  Top 2 features: MAE = 165223.28 ± 885.99\n",
      "  Top 3 features: MAE = 166706.51 ± 878.24\n",
      "  Top 4 features: MAE = 167255.68 ± 1179.86\n",
      "  Top 5 features: MAE = 167255.33 ± 1219.85\n",
      "  Top 6 features: MAE = 166821.05 ± 1161.37\n",
      "  Top 7 features: MAE = 166782.13 ± 1180.74\n"
     ]
    }
   ],
   "source": [
    "# Add as many cells as you need\n",
    "from sklearn.pipeline import Pipeline\n",
    "df['log_sqft'] = np.log1p(df['calculatedfinishedsquarefeet'])\n",
    "df['bed_bath_product'] = df['bedroomcnt'] * df['bathroomcnt']\n",
    "df['bed_to_room_ratio'] = df['bedroomcnt'] / (df['roomcnt'] + 1e-5)\n",
    "\n",
    "# Define all features (original + engineered)\n",
    "all_features = [\n",
    "    'bedroomcnt',\n",
    "    'bathroomcnt',\n",
    "    'roomcnt',\n",
    "    'calculatedfinishedsquarefeet',\n",
    "    'log_sqft',\n",
    "    'bed_bath_product',\n",
    "    'bed_to_room_ratio'\n",
    "]\n",
    "\n",
    "X = df[all_features]\n",
    "y = df['taxvaluedollarcnt']\n",
    "\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=42)\n",
    "\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(random_state=42),\n",
    "    \"Random Forest\": RandomForestRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "# Try selecting top k features for each model and evaluate\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    for k in range(2, len(all_features) + 1):\n",
    "        pipeline = Pipeline([\n",
    "            ('scale', StandardScaler()),\n",
    "            ('select', SelectKBest(score_func=f_regression, k=k)),\n",
    "            ('model', model)\n",
    "        ])\n",
    "        scores = cross_val_score(pipeline, X, y, scoring='neg_mean_absolute_error', cv=cv)\n",
    "        print(f\"  Top {k} features: MAE = {-scores.mean():.2f} ± {scores.std():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Discussion [3 pts]\n",
    "\n",
    "Analyze the effect of feature selection on your models:\n",
    "\n",
    "- Did performance improve for any models after reducing the number of features?\n",
    "\n",
    "- Which features were consistently retained across models?\n",
    "\n",
    "- Were any of your newly engineered features selected as important?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your text here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Fine-Tuning Your Three Models [6 pts]\n",
    "\n",
    "In this final phase of Milestone 2, you’ll select and refine your **three most promising models and their corresponding data pipelines** based on everything you've done so far, and pick a winner!\n",
    "\n",
    "1. For each of your three models:\n",
    "    - Choose your best engineered features and best selection of features as determined above. \n",
    "   - Perform hyperparameter tuning using `sweep_parameters`, `GridSearchCV`, `RandomizedSearchCV`, `Optuna`, etc. as you have practiced in previous homeworks. \n",
    "3. Decide on the best hyperparameters for each model, and for each run with repeated CV and record their final results:\n",
    "    - Report the **mean and standard deviation of CV MAE Score**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree (Best Params): {'model__min_samples_split': 5, 'model__min_samples_leaf': 2, 'model__max_depth': 5}\n",
      "  Mean MAE: 162112.21\n",
      "  Std Dev MAE: 701.85\n"
     ]
    }
   ],
   "source": [
    "dt_pipeline = Pipeline([\n",
    "    ('scale', StandardScaler()),\n",
    "    ('select', SelectKBest(score_func=f_regression, k=2)),\n",
    "    ('model', DecisionTreeRegressor(random_state=42))])\n",
    "\n",
    "dt_params = {'model__max_depth': [3, 5, 10, None],\n",
    "    'model__min_samples_split': [2, 5, 10],'model__min_samples_leaf': [1, 2, 4]}\n",
    "\n",
    "dt_search = RandomizedSearchCV(dt_pipeline, dt_params, n_iter=10, cv=cv,scoring='neg_mean_absolute_error', n_jobs=-1, random_state=42)\n",
    "dt_search.fit(X, y)\n",
    "\n",
    "best_dt = dt_search.best_estimator_\n",
    "dt_scores = -cross_val_score(best_dt, X, y, scoring='neg_mean_absolute_error', cv=cv)\n",
    "print(\"Decision Tree (Best Params):\", dt_search.best_params_)\n",
    "print(f\"  Mean MAE: {np.mean(dt_scores):.2f}\")\n",
    "print(f\"  Std Dev MAE: {np.std(dt_scores):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipeline = Pipeline([\n",
    "('scale', StandardScaler()),\n",
    "('select', SelectKBest(score_func=f_regression, k=2)),('model', RandomForestRegressor(random_state=42))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest (Best Params): {'model__n_estimators': 100, 'model__min_samples_split': 2, 'model__min_samples_leaf': 4, 'model__max_depth': 10}\n",
      "  Mean MAE: 161922.38\n",
      "  Std Dev MAE: 723.03\n"
     ]
    }
   ],
   "source": [
    "rf_params = {\n",
    "    'model__n_estimators': [100, 150, 200],\n",
    "    'model__max_depth': [None, 10, 20, 30],\n",
    "    'model__min_samples_split': [2, 5, 10],\n",
    "    'model__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "rf_search = RandomizedSearchCV(rf_pipeline, rf_params, n_iter=10, cv=cv,\n",
    "                               scoring='neg_mean_absolute_error', n_jobs=-1, random_state=42)\n",
    "rf_search.fit(X, y)\n",
    "\n",
    "best_rf = rf_search.best_estimator_\n",
    "rf_scores = -cross_val_score(best_rf, X, y, scoring='neg_mean_absolute_error', cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Mean MAE: 161922.38\n",
      "  Std Dev MAE: 723.03\n"
     ]
    }
   ],
   "source": [
    "print(f\"  Mean MAE: {np.mean(rf_scores):.2f}\")\n",
    "print(f\"  Std Dev MAE: {np.std(rf_scores):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression:\n",
      "  Mean MAE: 159537.35\n",
      "  Std Dev MAE: 718.87\n"
     ]
    }
   ],
   "source": [
    "lr_pipeline = Pipeline([\n",
    "    ('scale', StandardScaler()),\n",
    "    ('select', SelectKBest(score_func=f_regression, k=7)),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "lr_scores = cross_val_score(lr_pipeline, X, y, scoring='neg_mean_absolute_error', cv=cv)\n",
    "lr_mae = -lr_scores\n",
    "print(\"Linear Regression:\")\n",
    "print(f\"  Mean MAE: {np.mean(lr_mae):.2f}\")\n",
    "print(f\"  Std Dev MAE: {np.std(lr_mae):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression:\n",
      "  Mean MAE: 159537.35\n",
      "  Std Dev MAE: 718.87\n",
      "-\n",
      "Random Forest Regression:\n",
      "  Mean MAE: 161922.38\n",
      "  Std Dev MAE: 723.03\n",
      "-\n",
      "Decision Tree:\n",
      "  Mean MAE: 162112.21\n",
      "  Std Dev MAE: 701.85\n"
     ]
    }
   ],
   "source": [
    "print(\"Linear Regression:\")\n",
    "print(f\"  Mean MAE: {np.mean(lr_mae):.2f}\")\n",
    "print(f\"  Std Dev MAE: {np.std(lr_mae):.2f}\")\n",
    "print(\"-\")\n",
    "print(\"Random Forest Regression:\")\n",
    "print(f\"  Mean MAE: {np.mean(rf_scores):.2f}\")\n",
    "print(f\"  Std Dev MAE: {np.std(rf_scores):.2f}\")\n",
    "print(\"-\")\n",
    "print(\"Decision Tree:\")\n",
    "print(f\"  Mean MAE: {np.mean(dt_scores):.2f}\")\n",
    "print(f\"  Std Dev MAE: {np.std(dt_scores):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression:\n",
    "  Mean MAE: 159537.35\n",
    "\n",
    "  Std Dev MAE: 718.87\n",
    "\n",
    "Random Forest Regression:\n",
    "  Mean MAE: 161922.38\n",
    "\n",
    "  Std Dev MAE: 723.03\n",
    "\n",
    "Decision Tree:\n",
    "  Mean MAE: 162112.21\n",
    "  \n",
    "  Std Dev MAE: 701.85"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Discussion [3 pts]\n",
    "\n",
    "Reflect on your tuning process and final results:\n",
    "\n",
    "- What was your tuning strategy for each model? Why did you choose those hyperparameters?\n",
    "- Did you find that certain types of preprocessing or feature engineering worked better with specific models?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Linear Regression, I didn't perform traditional hyperparameter tuning, since the base model has no tunable parameters. Though, form problem 3, I did use feature selection using the best K found from problem 3, which was 7. For the Decision Tree Regression, I used RandomizedSearchCV to search over a range of hyperparameters, including a max_depth, min_samples_split, and min_samples_leaf. These were chosen because they directly control the model's complexity and tendency to overfit. For the Random Forest Regression, I again used RandomizedSearchCV, as the model has a larger hyperparameter space and randomized search is more computationally efficient than grid search. I tuned n_estimators, max_depth, min_samples_split, and min_samples_leaf. These hyperparameters affect the number of trees, tree complexity, and node splitting. \n",
    "I found that standardizing numerical features using StandardScaler helped improve performance, especially for Linear Regression. Also, using SelectKBest for feature selection was helpful for Linear Regression, as it helped from noise filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Final Model and Design Reassessment [6 pts]\n",
    "\n",
    "In this part, you will finalize your best-performing model.  You’ll also consolidate and present the key code used to run your model on the preprocessed dataset.\n",
    "**Requirements:**\n",
    "\n",
    "- Decide one your final model among the three contestants. \n",
    "\n",
    "- Below, include all code necessary to **run your final model** on the processed dataset, reporting\n",
    "\n",
    "    - Mean and standard deviation of CV MAE Score.\n",
    "    \n",
    "    - Test score on held-out test set. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use Linear Regression, as it turned out to be the best model to use with the lowest MAE Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add as many cells as you need\n",
    "X = df[['bedroomcnt','bathroomcnt','roomcnt','calculatedfinishedsquarefeet','log_sqft','bed_bath_product','bed_to_room_ratio']]\n",
    "y = df['taxvaluedollarcnt']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "final_pipeline = Pipeline([('scale', StandardScaler()),('select', SelectKBest(score_func=f_regression, k=7)),('model', LinearRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Linear Regression CV Results:\n",
      "  Mean MAE: 159947.98\n",
      "  Std Dev MAE: 1121.68\n"
     ]
    }
   ],
   "source": [
    "cv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "cv_scores = cross_val_score(final_pipeline, X_train, y_train,scoring='neg_mean_absolute_error', cv=cv)\n",
    "cv_mae = -cv_scores\n",
    "print(\"Final Linear Regression CV Results:\")\n",
    "print(f\"  Mean MAE: {np.mean(cv_mae):.2f}\")\n",
    "print(f\"  Std Dev MAE: {np.std(cv_mae):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set MAE: 157921.19\n"
     ]
    }
   ],
   "source": [
    "final_pipeline.fit(X_train, y_train)\n",
    "y_pred = final_pipeline.predict(X_test)\n",
    "test_mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nTest Set MAE: {test_mae:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Set MAE: 157921.19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Discussion [8 pts]\n",
    "\n",
    "In this final step, your goal is to synthesize your entire modeling process and assess how your earlier decisions influenced the outcome. Please address the following:\n",
    "\n",
    "1. Model Selection:\n",
    "- Clearly state which model you selected as your final model and why.\n",
    "\n",
    "- What metrics or observations led you to this decision?\n",
    "\n",
    "- Were there trade-offs (e.g., interpretability vs. performance) that influenced your choice?\n",
    "\n",
    "2. Revisiting an Early Decision\n",
    "\n",
    "- Identify one specific preprocessing or feature engineering decision from Milestone 1 (e.g., how you handled missing values, how you scaled or encoded a variable, or whether you created interaction or polynomial terms).\n",
    "\n",
    "- Explain the rationale for that decision at the time: What were you hoping it would achieve?\n",
    "\n",
    "- Now that you've seen the full modeling pipeline and final results, reflect on whether this step helped or hindered performance. Did you keep it, modify it, or remove it?\n",
    "\n",
    "- Justify your final decision with evidence—such as validation scores, visualizations, or model diagnostics.\n",
    "\n",
    "3. Lessons Learned\n",
    "\n",
    "- What insights did you gain about your dataset or your modeling process through this end-to-end workflow?\n",
    "\n",
    "- If you had more time or data, what would you explore next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your text here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
